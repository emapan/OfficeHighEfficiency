{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过snownlp库方法和朴素贝叶斯方法分别来对中文文本情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             comment  star\n0  口味：不知道是我口高了，还是这家真不怎么样。??我感觉口味确实很一般很一般。上菜相当快，我敢...     2\n1                                 菜品丰富质量好，服务也不错！很喜欢！     4\n2                        说真的，不晓得有人排队的理由，香精香精香精香精，拜拜！     2\n3  菜量实惠，上菜还算比较快，疙瘩汤喝出了秋日的暖意，烧茄子吃出了大阪烧的味道，想吃土豆片也是口...     5\n4  先说我算是娜娜家风荷园开业就一直在这里吃??每次出去回来总想吃一回??有时觉得外面的西式简餐...     4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>star</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>口味：不知道是我口高了，还是这家真不怎么样。??我感觉口味确实很一般很一般。上菜相当快，我敢...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>菜品丰富质量好，服务也不错！很喜欢！</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>说真的，不晓得有人排队的理由，香精香精香精香精，拜拜！</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>菜量实惠，上菜还算比较快，疙瘩汤喝出了秋日的暖意，烧茄子吃出了大阪烧的味道，想吃土豆片也是口...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>先说我算是娜娜家风荷园开业就一直在这里吃??每次出去回来总想吃一回??有时觉得外面的西式简餐...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#导入库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#读取数据\n",
    "data = pd.read_csv('data1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2, 4, 5, 1], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#评分的数值\n",
    "data['star'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将评分转化为二值变量\n",
    "def label(star):\n",
    "    if star > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['star_label'] = data.star.apply(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个第三库（snownlp），这个库可以直接对文本进行情感分析，返回的是积极性的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.8623218777387431 0.21406279508712744\n"
    }
   ],
   "source": [
    "#举例说明snownlp库的作用\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "text1 = '这个东西不错'\n",
    "text2 = '这个东西很垃圾'\n",
    "\n",
    "s1 = SnowNLP(text1)\n",
    "s2 = SnowNLP(text2)\n",
    "\n",
    "print(s1.sentiments,s2.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过snownlp库来确定情感得分\n",
    "def snownlp_result(comment_words):\n",
    "    s = SnowNLP(comment_words)\n",
    "    if s.sentiments >= 0.6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['sentiment'] = data.comment.apply(snownlp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.763\n"
    }
   ],
   "source": [
    "#对比star_label和sentiment两列的结果\n",
    "print((data[data['star_label']==data['sentiment']].shape[0])/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯方法用于情感得分预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\pr\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 0.777 seconds.\nPrefix dict has been built successfully.\n"
    }
   ],
   "source": [
    "#jieba分词\n",
    "import jieba\n",
    "\n",
    "def chinese_jieba_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "data['cut_comment'] = data.comment.apply(chinese_jieba_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据分为训练样本和测试样本\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['cut_comment']\n",
    "y = data.star_label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\Users\\pr\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n  'stop_words.' % sorted(inconsistent))\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   ipad  ok  ps  wifi  一下  一个个  一个半  一个多  一人  一份 ...  麻烦  麻辣  麻酱  麻麻  黄瓜  黄盖  \\\n0     0   0   0     0   0    0    0    0   0   0 ...   0   0   0   0   0   0   \n1     0   0   0     0   0    0    0    0   0   0 ...   0   0   0   0   0   0   \n2     0   0   0     0   0    0    0    0   0   0 ...   0   0   0   0   0   0   \n3     0   1   0     0   0    0    0    0   0   0 ...   1   0   0   0   0   0   \n4     0   0   0     0   0    0    0    0   0   0 ...   0   0   0   0   0   0   \n\n   黄色  黏糊糊  黑椒  默默  \n0   0    0   0   0  \n1   0    0   0   0  \n2   0    0   0   0  \n3   0    0   0   0  \n4   0    0   0   0  \n\n[5 rows x 1921 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ipad</th>\n      <th>ok</th>\n      <th>ps</th>\n      <th>wifi</th>\n      <th>一下</th>\n      <th>一个个</th>\n      <th>一个半</th>\n      <th>一个多</th>\n      <th>一人</th>\n      <th>一份</th>\n      <th>...</th>\n      <th>麻烦</th>\n      <th>麻辣</th>\n      <th>麻酱</th>\n      <th>麻麻</th>\n      <th>黄瓜</th>\n      <th>黄盖</th>\n      <th>黄色</th>\n      <th>黏糊糊</th>\n      <th>黑椒</th>\n      <th>默默</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1921 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#文本向量化\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_custom_stopwords(stopwords_file):\n",
    "    with open(stopwords_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "stopwords_file = '哈工大停用词表.txt'\n",
    "stopwords = get_custom_stopwords(stopwords_file)\n",
    "\n",
    "vect = CountVectorizer(max_df = 0.8, \n",
    "                       min_df = 3, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "vect_pd = pd.DataFrame(vect.fit_transform(X_train).toarray(), columns=vect.get_feature_names())\n",
    "vect_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.899375\n0.8275\n"
    }
   ],
   "source": [
    "#朴素贝叶斯方法模型\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "nb.fit(X_train_vect, y_train)\n",
    "train_score = nb.score(X_train_vect, y_train)\n",
    "print(train_score)\n",
    "\n",
    "X_test_vect = vect.transform(X_test)\n",
    "print(nb.score(X_test_vect, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = vect.transform(X)\n",
    "nb_result = nb.predict(X_vec)\n",
    "data['nb_result'] = nb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.885\n"
    }
   ],
   "source": [
    "#对比star_label和nb_result两列的结果\n",
    "print((data[data['star_label']==data['nb_result']].shape[0])/len(data))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}